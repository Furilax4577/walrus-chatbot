import { Worker } from 'bullmq';
import * as fs from 'fs';
import * as path from 'path';
import { ChatGateway } from '../chat/chat.gateway';
import { NestFactory } from '@nestjs/core';
import { AppModule } from '../app.module';
import { OpenaiService } from 'src/services/openai/openai.service';

let knowledgeChunks: string[] = [];

function loadKnowledgeFile() {
  const filePath = path.resolve(
    __dirname,
    '../../data/albatros_cleaned_filtered.txt',
  );
  try {
    const raw = fs.readFileSync(filePath, 'utf-8');
    knowledgeChunks = raw
      .split(/\n\s*\n/) // SÃ©parer par paragraphes
      .map((chunk) => chunk.trim())
      .filter((chunk) => chunk.length > 0);
    console.log(`ğŸ“š ${knowledgeChunks.length} blocs de connaissance chargÃ©s.`);
  } catch (error) {
    console.error(
      'âŒ Erreur lors du chargement du fichier de connaissance :',
      error.message,
    );
  }
}

function getRelevantChunks(userInput: string, limit = 3): string[] {
  const input = userInput.toLowerCase();
  const scores = knowledgeChunks.map((chunk) => {
    let score = 0;
    input.split(/\s+/).forEach((word) => {
      if (chunk.toLowerCase().includes(word)) score++;
    });
    return { chunk, score };
  });

  return scores
    .sort((a, b) => b.score - a.score)
    .slice(0, limit)
    .map((s) => s.chunk);
}

export async function startChatWorker(gateway: ChatGateway) {
  loadKnowledgeFile();

  const app = await NestFactory.createApplicationContext(AppModule);
  const openaiService = app.get(OpenaiService);

  const worker = new Worker(
    'chat',
    async (job) => {
      const { clientId, messages } = job.data;
      console.log(`ğŸ¤– Traitement pour client ${clientId}`);

      try {
        const lastUserMessage = messages[messages.length - 1]?.content ?? '';
        const contextChunks = getRelevantChunks(lastUserMessage);

        if (contextChunks.length === 0) {
          console.log(
            `ğŸ“­ Aucun contexte trouvÃ© pour ${clientId}, redirection humaine.`,
          );
          const message =
            "Nous n'avons pas trouvÃ© de rÃ©ponse automatique Ã  votre question. Votre message a Ã©tÃ© transmis Ã  notre Ã©quipe, nous reviendrons vers vous rapidement.";
          gateway.sendMessageToClient(clientId, message);
          return message;
        }

        const augmentedMessages = [
          {
            role: 'system',
            content: `Tu es un assistant pour le client ${clientId}. Voici des informations utiles :\n\n${contextChunks.join('\n\n')}\n\nTu dois rÃ©pondre :\n- en franÃ§ais uniquement\n- en Markdown clair (titres, listes, gras, liensâ€¦)\n- de faÃ§on concise (3 Ã  6 phrases max)\n- et si besoin, poser une question pour mieux affiner la rÃ©ponse.`,
          },
          ...messages,
        ];

        const message =
          await openaiService.generateChatResponse(augmentedMessages);

        console.log(`ğŸ’¬ RÃ©ponse pour ${clientId} :`, message);
        gateway.sendMessageToClient(clientId, message);

        return message;
      } catch (error) {
        console.error(
          `âŒ Erreur pendant le traitement du job pour ${clientId} :`,
          error.message,
        );
        gateway.sendErrorToClient(clientId, error.message);
        throw new Error(
          `Erreur de traitement pour ${clientId} : ${error.message}`,
        );
      }
    },
    {
      connection: {
        host: process.env.REDIS_HOST || 'localhost',
        port: 6379,
      },
    },
  );

  worker.on('completed', (job, result) => {
    console.log(`âœ… Job ${job.id} terminÃ© : ${result}`);
  });

  worker.on('failed', (job, err) => {
    console.error(`ğŸ›‘ Job ${job?.id} Ã©chouÃ© :`, err.message);
  });
}
